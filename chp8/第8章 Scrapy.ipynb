{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple\n",
      "Requirement already satisfied: scrapy in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: parsel>=1.5.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cryptography>=2.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (35.0.0)\n",
      "Requirement already satisfied: h2<4.0,>=3.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (21.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (21.2.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (4.6.3)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from h2<4.0,>=3.0->scrapy) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from h2<4.0,>=3.0->scrapy) (5.2.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyasn1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (21.2.0)\n",
      "Requirement already satisfied: twisted-iocpsupport~=1.0.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: priority<2.0,>=1.1.0 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (36.4.0)\n",
      "Requirement already satisfied: pycparser in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: idna>=2.5 in d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 2.5.1 - no active project\n",
      "\n",
      "Usage:\n",
      "  scrapy <command> [options] [args]\n",
      "\n",
      "Available commands:\n",
      "  bench         Run quick benchmark test\n",
      "  commands      \n",
      "  fetch         Fetch a URL using the Scrapy downloader\n",
      "  genspider     Generate new spider using pre-defined templates\n",
      "  runspider     Run a self-contained spider (without creating a project)\n",
      "  settings      Get settings values\n",
      "  shell         Interactive scraping console\n",
      "  startproject  Create new project\n",
      "  version       Print Scrapy version\n",
      "  view          Open URL in browser, as seen by Scrapy\n",
      "\n",
      "  [ more ]      More commands available when run from project directory\n",
      "\n",
      "Use \"scrapy <command> -h\" to see more info about a command\n"
     ]
    }
   ],
   "source": [
    "!scrapy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# !scrapy startproject example\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\0ilraypan\\git_jia\\《用Python写网络爬虫2》\\chp8\\example\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\0ilraypan\\git_jia\\《用Python写网络爬虫2》\\chp8\\example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 D 中的卷是 DATA1\n",
      " 卷的序列号是 665B-B6E5\n",
      "\n",
      " D:\\0ilraypan\\git_jia\\《用Python写网络爬虫2》\\chp8\\example 的目录\n",
      "\n",
      "2021/10/31  18:28    <DIR>          .\n",
      "2021/10/31  18:28    <DIR>          ..\n",
      "2021/11/02  11:08    <DIR>          example\n",
      "2021/10/30  09:10               269 scrapy.cfg\n",
      "               1 个文件            269 字节\n",
      "               3 个目录 501,673,762,816 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2.2创建爬虫"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "# !scrapy genspider country_or_district example.python-scraping.com"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 11:25:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://example.python-scraping.com/robots.txt> (failed 3 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.\n",
      "2021-11-02 11:25:44 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://example.python-scraping.com/robots.txt>: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages\\twisted\\internet\\defer.py\", line 47, in run\n",
      "    return f(*args, **kwargs)\n",
      "  File \"d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 44, in process_request\n",
      "    return (yield download_func(request=request, spider=spider))\n",
      "twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.\n",
      "2021-11-02 11:26:00 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET http://example.python-scraping.com/> (failed 3 times): Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.\n",
      "2021-11-02 11:26:00 [scrapy.core.scraper] ERROR: Error downloading <GET http://example.python-scraping.com/>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages\\twisted\\internet\\defer.py\", line 47, in run\n",
      "    return f(*args, **kwargs)\n",
      "  File \"d:\\0ilraypan\\python_work\\env\\hack\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 44, in process_request\n",
      "    return (yield download_func(request=request, spider=spider))\n",
      "twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: 由于目标计算机积极拒绝，无法连接。.\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl country_or_district -s LOG_LEVEL=ERROR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!scrapy shell http://example.python-scraping.com/view/United-Kingdom-233\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前的 WinHTTP 代理服务器设置:\n",
      "\n",
      "    代理服务器:  192.168.5.1:7890\n",
      "    绕过列表     :  localhost;127.*;10.*;172.16.*;172.17.*;172.18.*;172.19.*;172.20.*;172.21.*;172.22.*;172.23.*;172.24.*;172.25.*;172.26.*;172.27.*;172.28.*;172.29.*;172.30.*;172.31.*;127.0.0.1;<local>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!netsh winhttp show proxy\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}